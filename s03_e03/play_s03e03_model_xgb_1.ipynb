{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaideepmurkute/Active-Learning-Supervised-Machine-Learning-With-Minimal-Labeled-Data/blob/master/s03_e03/play_s03e03_model_xgb_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ec7wkQrQzTZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f804f0f-8239-45d6-b195-3d3e6e8baa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb -U -qqq                                           \n",
        "! pip install sklearn -U -qqq\n",
        "! pip install xgboost==1.6.0 #-U -qqq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzjUcHUpQ7C5",
        "outputId": "18f9451f-2711-4fd8-be4c-6e7fc80bf231"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost==1.6.0 in /usr/local/lib/python3.8/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.6.0) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "xgboost.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FUednWRAZ6l1",
        "outputId": "41a2f115-86f4-4b2d-c44c-fb0dd34aecb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "from sklearn.datasets import fetch_california_housing \n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.linear_model import LinearRegression, SGDOneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "# from sklearn.neighbors import LocalOutlierFactor\n",
        "# from sklearn.metrics import rmse\n",
        "\n",
        "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
        "from sklearn.datasets import fetch_california_housing \n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
        "\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "\n",
        "from scipy.stats.mstats import winsorize\n",
        "from scipy.stats import mode\n",
        "\n",
        "import wandb\n",
        "\n",
        "# import reverse_geocoder\n",
        "# import geopy\n",
        "# import category_encoders as ce\n",
        "\n"
      ],
      "metadata": {
        "id": "_nZvi96Dzcpj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seeds(config):\n",
        "  np.random.seed(config[\"random_state\"])\n",
        "  random.seed(config[\"random_state\"])\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(config[\"random_state\"])\n",
        "  '''\n",
        "  torch.manual_seed(config[\"random_state\"])\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(config[\"random_state\"])\n",
        "      torch.cuda.manual_seed_all(config[\"random_state\"])\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = True \n",
        "  '''\n",
        "\n",
        "\n",
        "def generate_fold_idx(config, train_df, group_col=None):\n",
        "  if config['fold_split_type'] == 'kfold':\n",
        "    splitter = KFold(n_splits=config['num_folds'], shuffle=True, \n",
        "                                     random_state=config['random_state'])\n",
        "  elif config['fold_split_type'] == 'strat_kfold':\n",
        "    splitter = StratifiedKFold(n_splits=config['num_folds'], shuffle=True, \n",
        "                                     random_state=config['random_state'])\n",
        "  elif config['fold_split_type'] == 'group_kfold':\n",
        "    splitter = GroupKFold(n_splits=config['num_folds'])\n",
        "  else:\n",
        "    raise ValueError(\"fold_split_type {} not recognized... Choose from: \\\n",
        "                    time_series_split, group_time_series_split, purged_time_series_split, kfold\")\n",
        "  \n",
        "  fold_idx_dict = dict()\n",
        "  if config['fold_split_type'] == 'group_kfold':\n",
        "    if group_col in train_df.columns:\n",
        "      for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X=train_df, \n",
        "                                                                    groups=train_df[group_col].values)):\n",
        "        fold_idx_dict[fold_idx] = dict()\n",
        "        fold_idx_dict[fold_idx]['train_idx'] = train_idx\n",
        "        fold_idx_dict[fold_idx]['val_idx'] = val_idx\n",
        "  else:\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(splitter.split(X=train_df, y=train_df.Attrition.values)):\n",
        "      fold_idx_dict[fold_idx] = dict()\n",
        "      fold_idx_dict[fold_idx]['train_idx'] = train_idx\n",
        "      fold_idx_dict[fold_idx]['val_idx'] = val_idx\n",
        "  \n",
        "  return fold_idx_dict\n",
        "\n",
        "\n",
        "def save_model(config, model):\n",
        "  model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.xgb'\n",
        "  \n",
        "  model_local_save_path = config['local_model_dir'] + '/' + model_save_fname\n",
        "  model_drive_save_path = config['drive_model_dir'] + '/' + model_save_fname\n",
        "\n",
        "  print('Saving model...')\n",
        "  model.save_model(model_local_save_path)\n",
        "\n",
        "  print('Copying model to drive...')\n",
        "  shutil.copy(model_local_save_path, model_drive_save_path)\n",
        "  \n",
        "\n",
        "def load_model(config):\n",
        "  model_save_fname = config['model_name'] + '_fold_' + str(config['curr_fold']) + '.xgb'\n",
        "  \n",
        "  model_local_save_path = config['local_model_dir'] + '/' + model_save_fname\n",
        "  model_drive_save_path = config['drive_model_dir'] + '/' + model_save_fname\n",
        "\n",
        "  if not os.path.exists(model_local_save_path):\n",
        "    shutil.copy(model_drive_save_path, model_local_save_path)\n",
        "  \n",
        "  print('Loading model...')\n",
        "  model = xgb.XGBClassifier()\n",
        "  model.load_model(model_local_save_path)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def get_xgb_params(config):\n",
        "  xgb_params = {\n",
        "            'random_state': config['random_state'], \n",
        "            'n_jobs': config['n_jobs'], \n",
        "            'verbosity': config['verbosity'], \n",
        "            \n",
        "            'tree_method': config['tree_method'], \n",
        "            'max_depth': config['max_depth'], \n",
        "            'max_leaves': config['max_leaves'], \n",
        "            'n_estimators': config['n_estimators'], \n",
        "            'early_stopping_rounds': config['early_stopping_rounds'], \n",
        "            \n",
        "            'colsample_bytree': config['colsample_bytree'], \n",
        "            'subsample': config['subsample'], \n",
        "            'reg_alpha': config['reg_alpha'], \n",
        "            'reg_lambda': config['reg_lambda'], \n",
        "            'enable_categorical': config['enable_categorical'], \n",
        "            \n",
        "            'learning_rate': config['learning_rate'], \n",
        "            'objective': config['objective'], \n",
        "            'eval_metric': config['eval_metric'],\n",
        "            \n",
        "            # 'num_class': config['num_classes'],\n",
        "        }\n",
        "  \n",
        "  return xgb_params\n",
        "\n"
      ],
      "metadata": {
        "id": "IpZBnKwWzcm7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_data(config):\n",
        "  '''\n",
        "  data_type : feature / orig\n",
        "  '''\n",
        "  if config['feature_version'] == 'v0':\n",
        "    for fname in ['train.csv', 'test.csv', 'sample_submission.csv']:\n",
        "      shutil.copy(os.path.join(config['drive_data_dir'], fname), \n",
        "                  os.path.join(config['local_data_dir'], fname)\n",
        "                  )\n",
        "    train_df = pd.read_csv(os.path.join(config['local_data_dir'], 'train.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(config['local_data_dir'], 'test.csv'))\n",
        "    sub_df = pd.read_csv(os.path.join(config['local_data_dir'], 'sample_submission.csv'))\n",
        "\n",
        "    train_df.drop(['id'], axis=1, inplace=True)\n",
        "    \n",
        "    if config['include_orig_data']:\n",
        "      for fname in ['orig_data.csv']:\n",
        "        shutil.copy(os.path.join(config['drive_data_dir'], fname), \n",
        "                    os.path.join(config['local_data_dir'], fname)\n",
        "                    )\n",
        "      orig_data_df = pd.read_csv(os.path.join(config['local_data_dir'], 'orig_data.csv'))\n",
        "      orig_data_df['Attrition'] = orig_data_df['Attrition'].replace('Yes', 1).replace('No', 0)\n",
        "      orig_data_df.drop(['EmployeeNumber'], axis=1, inplace=True)\n",
        "\n",
        "      train_df = pd.concat((train_df, orig_data_df), axis=0)\n",
        "      \n",
        "  else:\n",
        "    train_feature_fname = 'train_features_' + config['feature_version'] + '.csv'\n",
        "    test_feature_fname = 'test_features_' + config['feature_version'] + '.csv'\n",
        "    for fname in [train_feature_fname, test_feature_fname]:\n",
        "      shutil.copy(os.path.join(config['drive_feature_dir'], fname), \n",
        "                  os.path.join(config['local_feature_dir'], fname)\n",
        "                  )\n",
        "    \n",
        "    for fname in ['train.csv', 'test.csv', 'sample_submission.csv']:\n",
        "      shutil.copy(os.path.join(config['drive_data_dir'], fname), \n",
        "                  os.path.join(config['local_data_dir'], fname)\n",
        "                  )\n",
        "    train_df = pd.read_csv(os.path.join(config['local_feature_dir'], train_feature_fname))\n",
        "    test_df = pd.read_csv(os.path.join(config['local_feature_dir'], test_feature_fname))\n",
        "    sub_df = pd.read_csv(os.path.join(config['local_data_dir'], 'sample_submission.csv'))\n",
        "      \n",
        "  \n",
        "  return train_df, test_df, sub_df\n",
        "  \n",
        "\n",
        "def encode_features(config, train_df, test_df):\n",
        "  \n",
        "  config['nominal_categorical_cols'] = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime']\n",
        "  config['ordinal_categorical_cols'] = ['BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', \n",
        "                              'PerformanceRating',  'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear',\n",
        "                              'WorkLifeBalance']\n",
        "  config['ordinal_continuous_cols'] = ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', \n",
        "                             'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', \n",
        "                             'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
        "\n",
        "  # one-hot encoding for nominal_categorical_cols\n",
        "  train_df_encoded = pd.get_dummies(train_df, columns=config['nominal_categorical_cols'], prefix=config['nominal_categorical_cols'])\n",
        "  test_df_encoded = pd.get_dummies(test_df, columns=config['nominal_categorical_cols'], prefix=config['nominal_categorical_cols'])\n",
        "  \n",
        "  # ---------------\n",
        "\n",
        "  # Convert ordinal_categorical column BusinessTravel, with non-numeric values, to numeric values\n",
        "  BusinessTravel_map = {'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}\n",
        "  train_df_encoded.BusinessTravel = train_df_encoded.BusinessTravel.map(BusinessTravel_map)\n",
        "  test_df_encoded.BusinessTravel = test_df_encoded.BusinessTravel.map(BusinessTravel_map)\n",
        "\n",
        "  # ---------------\n",
        "\n",
        "  # Handle case of if some categories/values are missing in the test set; so columns mismatch\n",
        "  missing_cols = set( train_df_encoded.columns ) - set( test_df_encoded.columns )\n",
        "  print(\"missing_cols: \", missing_cols)\n",
        "  # Add a missing column in test set with default value equal to 0\n",
        "  for c in missing_cols: test_df_encoded[c] = 0\n",
        "  \n",
        "  # Ensure the order of column in the test set is in the same order than in train set\n",
        "  test_df_encoded = test_df_encoded[train_df_encoded.columns]\n",
        "  \n",
        "  for col in train_df_encoded.columns:\n",
        "    if col not in test_df_encoded.columns:\n",
        "      print(\"col {} present in train_df_encoded but not in test_df_encoded...\")\n",
        "      raise\n",
        "  \n",
        "  # -----------\n",
        "  # -----------\n",
        "  \n",
        "  config['encoded_nominal_categorical_cols'] = []\n",
        "  for orig_nom_cat_col in config['nominal_categorical_cols']:\n",
        "    for col in train_df_encoded.columns:\n",
        "      if col.startswith(orig_nom_cat_col+'_'):\n",
        "        config['encoded_nominal_categorical_cols'].append(col)\n",
        "  print(\"config['encoded_nominal_categorical_cols']: \", config['encoded_nominal_categorical_cols'])\n",
        "  \n",
        "\n",
        "  # dupl_np_encoded_nominal_categorical_cols = []\n",
        "  # np_encoded_nominal_categorical_cols = np.array(config['encoded_nominal_categorical_cols'])\n",
        "  # for col in np_encoded_nominal_categorical_cols:\n",
        "  #   if len(np_encoded_nominal_categorical_cols[np_encoded_nominal_categorical_cols==col]) > 1:\n",
        "  #     dupl_np_encoded_nominal_categorical_cols.append(col)\n",
        "  # print(\"dupl_np_encoded_nominal_categorical_cols: \", dupl_np_encoded_nominal_categorical_cols)\n",
        "  # raise\n",
        "\n",
        "\n",
        "  config['encoded_ordinal_categorical_cols'] = []\n",
        "  for orig_ord_cat_col in config['ordinal_categorical_cols']:\n",
        "    for col in train_df_encoded.columns:\n",
        "      if col.startswith(orig_ord_cat_col+'_'):\n",
        "        config['encoded_ordinal_categorical_cols'].append(col)\n",
        "  print(\"config['encoded_ordinal_categorical_cols']: \", config['encoded_ordinal_categorical_cols'])\n",
        "  \n",
        "  # -----------\n",
        "\n",
        "  return train_df_encoded, test_df_encoded\n",
        "  \n",
        "\n",
        "\n",
        "def get_feature_cols(config, train_df):\n",
        "  config['id_cols'] = ['id']\n",
        "\n",
        "  config['cont_cols'] = config['ordinal_continuous_cols']\n",
        "  \n",
        "  '''\n",
        "  # -------------------\n",
        "  dupl_encoded_nominal_categorical_cols = []\n",
        "  np_feature_cols = np.array(config['encoded_nominal_categorical_cols'])\n",
        "  for col in np_feature_cols:\n",
        "    if len(np_feature_cols[np_feature_cols == col]) > 1:\n",
        "      dupl_encoded_nominal_categorical_cols.append(col)\n",
        "  print(\"dupl_encoded_nominal_categorical_cols: \", dupl_encoded_nominal_categorical_cols)\n",
        "  # ----------------------\n",
        "  dupl_encoded_ordinal_categorical_cols = []\n",
        "  np_feature_cols = np.array(config['encoded_ordinal_categorical_cols'])\n",
        "  for col in np_feature_cols:\n",
        "    if len(np_feature_cols[np_feature_cols == col]) > 1:\n",
        "      dupl_encoded_ordinal_categorical_cols.append(col)\n",
        "  print(\"dupl_encoded_ordinal_categorical_cols: \", dupl_encoded_ordinal_categorical_cols)\n",
        "  # ----------------------\n",
        "  overlap_cols = []\n",
        "  for col in config['encoded_nominal_categorical_cols']:\n",
        "    if col in config['encoded_ordinal_categorical_cols']:\n",
        "      overlap_cols.append(col)\n",
        "  # print(f\"col: {col} is in both encoded_nominal_categorical_cols & encoded_ordinal_categorical_cols\")\n",
        "  print(\"cols in both type of categorical features: \", overlap_cols)\n",
        "  # ----------------------\n",
        "\n",
        "  raise\n",
        "  '''\n",
        "  config['cat_cols'] = config['encoded_nominal_categorical_cols'] + config['encoded_ordinal_categorical_cols']\n",
        "\n",
        "  config['target_cols'] = ['Attrition']\n",
        "\n",
        "  config['feature_cols'] = config['cont_cols'] + config['cat_cols']\n",
        "\n",
        "  return config\n",
        "\n"
      ],
      "metadata": {
        "id": "eVyxTmmV1mTT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_data(train_df, test_df, cols, method):\n",
        "  if method == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "  elif method == 'robust':\n",
        "    scaler = RobustScaler()\n",
        "  \n",
        "  scaler.fit(train_df[cols].values)\n",
        "  train_df[cols] = scaler.transform(train_df[cols])\n",
        "  test_df[cols] = scaler.transform(test_df[cols])\n",
        "\n",
        "  return train_df, test_df\n",
        "    "
      ],
      "metadata": {
        "id": "-PDWoJqBpN6x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def IQR_outlier_handling(train_df, test_df, cols, handling_type):\n",
        "  for col in cols:\n",
        "    # calculate interquartile range\n",
        "    q25, q75 = np.percentile(train_df[col].values, 25), np.percentile(train_df[col].values, 75)\n",
        "    iqr = q75 - q25\n",
        "    \n",
        "    # calculate the outlier cutoff\n",
        "    cut_off = iqr * 1.5\n",
        "    lower_cutoff, upper_cutoff = q25 - cut_off, q75 + cut_off\n",
        "    \n",
        "    num_outliers = train_df[col].loc[(train_df[col] < lower_cutoff) | (train_df[col] > upper_cutoff)].shape[0]\n",
        "    print(\"col: {} \\t # num_outliers: {}\".format(col, num_outliers))\n",
        "\n",
        "    if handling_type == 'remove_train_clip_test':\n",
        "      train_df[col] = train_df[col].loc[(not(train_df[col] < lower_cutoff)) & (not(train_df[col] > upper_cutoff))]\n",
        "      if col in test_df.columns:\n",
        "        test_df[col].loc[test_df[col] < lower_cutoff] = lower_cutoff\n",
        "        test_df[col].loc[test_df[col] > upper_cutoff] = upper_cutoff\n",
        "    elif handling_type == 'clip':\n",
        "      train_df[col].loc[train_df[col] < lower_cutoff] = lower_cutoff\n",
        "      train_df[col].loc[train_df[col] > upper_cutoff] = upper_cutoff\n",
        "      if col in test_df.columns:\n",
        "        test_df[col].loc[test_df[col] < lower_cutoff] = lower_cutoff\n",
        "        test_df[col].loc[test_df[col] > upper_cutoff] = upper_cutoff\n",
        "      \n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def winsorize(train_df, test_df, cols, lower_lim=0.01, upper_lim=0.98):\n",
        "  # lower_lim = train_df.quantile(0.01)\n",
        "  # upper_lim = train_df.quantile(0.99)\n",
        "  for col in cols:\n",
        "    train_df[col] = winsorize(train_df[col], (lower_lim, upper_lim))\n",
        "    test_df[col] = winsorize(test_df[col], (lower_lim, upper_lim))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "def isolation_forest_outlier_handling(train_df, test_df, cols, outlier_thresh=-0.1, \n",
        "                                      handling_method='drop_median', seed=0):\n",
        "  print(\"Training Isolation forest model to detect outliers...\")\n",
        "  iso_forest_model = IsolationForest(n_estimators=500, contamination='auto', random_state=seed)\n",
        "  iso_forest_model.fit(train_df[cols], train_df.MedHouseVal.values)\n",
        "  \n",
        "  sample_scores_train = iso_forest_model.decision_function(train_df[cols])\n",
        "  sample_scores_test = iso_forest_model.decision_function(test_df[cols])\n",
        "\n",
        "  print(\"# train outliers: \", np.sum(sample_scores_train < outlier_thresh))\n",
        "  print(\"# test outliers: \", np.sum(sample_scores_test < outlier_thresh))\n",
        "  \n",
        "  if handling_method == 'drop_median':\n",
        "    print(\"Dropping outlier train samples...\")\n",
        "    # drop train samples and replace test sample values with median from train columns\n",
        "    train_df = train_df.loc[sample_scores_train >= outlier_thresh]\n",
        "    \n",
        "    print(\"Clipping outlier test samples to median value...\")\n",
        "    for col in cols:\n",
        "      test_df[col].loc[sample_scores_test < outlier_thresh] = train_df[col].median(axis=0)\n",
        "  elif handling_method == 'winsorize':\n",
        "    train_df.loc[sample_scores_train < outlier_thresh] = winsorize(train_df.loc[sample_scores_train < outlier_thresh], \n",
        "                                                                  cols, lower_lim=0.01, upper_lim=0.98)\n",
        "    test_df.loc[sample_scores_test < outlier_thresh] = winsorize(test_df.loc[sample_scores_test < outlier_thresh], \n",
        "                                                                  cols, lower_lim=0.01, upper_lim=0.98)\n",
        "    \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "def handle_outliers(config, train_df, test_df, cols, method):\n",
        "  \n",
        "  if method == 'winsorize':\n",
        "    train_df, test_df = winsorize(train_df, test_df, cols=cols)\n",
        "  elif method == 'iso_forest':\n",
        "    train_df, test_df = isolation_forest_outlier_handling(train_df, test_df, cols=cols, \n",
        "                                  outlier_thresh=-0.1, handling_method='drop_median', seed=config['seed'])\n",
        "  elif method == 'iqr':\n",
        "    train_df, test_df = IQR_outlier_handling(train_df, test_df, cols, handling_type='clip')\n",
        "\n",
        "  return train_df, test_df\n"
      ],
      "metadata": {
        "id": "Ul39e-gKyZga"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rmse(label, pred):\n",
        "  return np.sqrt(np.mean((label-pred)**2))\n",
        "\n",
        "def mse(label, pred):\n",
        "  return np.mean((label-pred)**2)\n",
        "\n",
        "def get_config():\n",
        "  return config\n",
        "\n",
        "def save_config(config):\n",
        "  config_to_save = {}  # to avoid types like object or others that somtimes cause problem reading data back.\n",
        "  for k, v in config.items():\n",
        "    if isinstance(v, (bool, int, float, str, list, dict)):\n",
        "      config_to_save[k] = v\n",
        "  \n",
        "  config_local_save_path = os.path.join(config['local_model_dir'], 'saved_config.json')\n",
        "  config_drive_save_path = os.path.join(config['drive_model_dir'], 'saved_config.json')\n",
        "  \n",
        "  with open(config_local_save_path, 'w') as fp:\n",
        "    json.dump(config_to_save, fp, indent=4, sort_keys=True)\n",
        "  \n",
        "  shutil.copy(config_local_save_path, config_drive_save_path)\n",
        "\n",
        "\n",
        "def train_k_folds():\n",
        "\n",
        "  # needed becuse variable that is updated within function becomes a local variable and has to be passed in.\n",
        "  config = get_config()\n",
        "  # print(\"1\")\n",
        "  print(\"config: \", config)\n",
        "\n",
        "  create_paths(config)\n",
        "  # print(\"2\")\n",
        "  train_df, test_df, sub_df = get_data(config)\n",
        "  print(\"train_df.shape: \", train_df.shape)\n",
        "  \n",
        "  train_df, test_df = encode_features(config, train_df, test_df)\n",
        "\n",
        "  config = get_feature_cols(config, train_df)\n",
        "  print(\"config['feature_cols']: \", config['feature_cols'])\n",
        "\n",
        "  \n",
        "  \n",
        "  # dupl_cols = []\n",
        "  # np_feature_cols = np.array(config['feature_cols'])\n",
        "  # for col in np_feature_cols:\n",
        "  #   if len(np_feature_cols[np_feature_cols == col]) > 1:\n",
        "  #     dupl_cols.append(col)\n",
        "  # print(\"dupl_cols: \", dupl_cols)\n",
        "\n",
        "  # -------------\n",
        "\n",
        "  # if config['handle_outliers']:\n",
        "  #   # handle outliers\n",
        "  #   print(\"Before outlier handling: \")\n",
        "  #   print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  #   train_df, test_df = handle_outliers(config, train_df, test_df, cols=float_feature_cols, method='iqr')\n",
        "  #   print(\"After outlier handling: \")\n",
        "  #   print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  \n",
        "  # if config['scale_data']:\n",
        "  #   print(\"Scaling data...\")\n",
        "  #   # scale data\n",
        "  #   # float_feature_cols = [col for col in config['feature_cols'] if col not in ['Latitude', 'Longitude', 'MedHouseVal']]\n",
        "  #   train_df, test_df = scale_data(train_df, test_df, cols=float_feature_cols, method='robust')\n",
        "\n",
        "  # ------------------\n",
        "  \n",
        "  fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  per_model_metrics = {\n",
        "                        'AUROC': {'train': [], 'val': []},\n",
        "                        'f1_score': {'train': [], 'val': []},\n",
        "                        'logloss': {'train': [], 'val': []},\n",
        "                      }\n",
        "  \n",
        "  shutil.copy('/content/drive/MyDrive/Playground Series/S03_E03/code/play_s03e03_model_xgb_1.ipynb', \n",
        "              os.path.join(config['drive_model_dir'], 'model_2.ipynb'))\n",
        "  save_config(config)\n",
        "\n",
        "  # fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "  # if config['use_wandb']:\n",
        "  wandb.init(name=config['model_name'], project=config['project_name'], \n",
        "            tags=['baseline'], config=config)\n",
        "  if config['choice'] == 3:\n",
        "    print(\"Updating sweep configs...\")\n",
        "    for k, v in wandb.config.items():\n",
        "      config[k] = v\n",
        "    print(\"*** Updated sweep config: \", config)\n",
        "    \n",
        "  for fold_num in range(config['num_folds']):\n",
        "    if fold_num not in config['folds_to_train']:\n",
        "      continue\n",
        "\n",
        "    print(\"Training fold: \", fold_num)\n",
        "    config['curr_fold'] = fold_num\n",
        "\n",
        "    # -----------\n",
        "    # fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "    # # if config['use_wandb']:\n",
        "    # wandb.init(name=fold_model_name, project=config['project_name'], \n",
        "    #           tags=['baseline'], config=config)\n",
        "    # if config['choice'] == 3:\n",
        "    #   print(\"Updating sweep configs...\")\n",
        "    #   for k, v in wandb.config.items():\n",
        "    #     config[k] = v\n",
        "    #   print(\"*** Updated sweep config: \", config)\n",
        "    \n",
        "    set_seeds(config)\n",
        "    \n",
        "    # -----------\n",
        "    \n",
        "    train_idx = fold_idx_dict[fold_num]['train_idx']\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    print(\"len(train_idx): {} \\t len(val_idx): {}\".format(len(train_idx), len(val_idx)))\n",
        "    \n",
        "    # print(\"config['feature_cols']: \", config['feature_cols'])\n",
        "    train_data = train_df[config['feature_cols']].iloc[train_idx]\n",
        "    train_label = train_df[config['target_cols']].iloc[train_idx]\n",
        "\n",
        "    val_data = train_df[config['feature_cols']].iloc[val_idx]\n",
        "    val_label = train_df[config['target_cols']].iloc[val_idx]\n",
        "\n",
        "    xgb_params = get_xgb_params(config)\n",
        "    if config['scale_pos_weight'] == 'auto':\n",
        "      auto_pos_cls_weight = train_label[train_label.values==0].shape[0] / train_label[train_label.values==1].shape[0]\n",
        "      print(\"Setting scale_pos_weight to: \", auto_pos_cls_weight)\n",
        "      xgb_params['scale_pos_weight'] = auto_pos_cls_weight\n",
        "    else:\n",
        "      xgb_params['scale_pos_weight'] = config['scale_pos_weight']\n",
        "    \n",
        "\n",
        "    model = xgb.XGBClassifier(**xgb_params)\n",
        "\n",
        "    # print(\"train_data.shape: \", train_data.shape)\n",
        "    # print(\"train_label.shape: \", train_label.shape)\n",
        "    # print(\"train_label[:10]: \", train_label[:10])\n",
        "    \n",
        "    # print(\"val_data.shape: \", val_data.shape)\n",
        "    # print(\"val_label.shape: \", val_label.shape)\n",
        "    # print(\"val_label[:10]: \", val_label[:10])\n",
        "    \n",
        "    print(\"Training model ...\")\n",
        "    model.fit(train_data, train_label, \n",
        "              # early_stopping_rounds=config['early_stopping_rounds'], \n",
        "              # eval_metric=config['eval_metric'],\n",
        "              eval_set=[(train_data, train_label), \n",
        "                        (val_data, val_label)], \n",
        "              verbose=50, \n",
        "              )\n",
        "    \n",
        "    print(\"Saving Model...\")\n",
        "    save_model(config, model)\n",
        "\n",
        "    # -----------\n",
        "    # print(model.evals_result())\n",
        "    for i in range(len(model.evals_result()['validation_0'][config['eval_metric']])):\n",
        "      wandb.log({\n",
        "        \"Per Epoch Train AUC \": model.evals_result()['validation_0'][config['eval_metric']][i], \n",
        "        \"Per Epoch Val AUC\": model.evals_result()['validation_1'][config['eval_metric']][i], \n",
        "        }\n",
        "      )\n",
        "    # raise\n",
        "    # print(\"Best val epoch: min validation_0 auc: \", np.min(model.evals_result()['validation_0'][config['eval_metric']]))\n",
        "    \n",
        "    \n",
        "    print(\"Best val epoch: min validation_1 auc: \", np.min(model.evals_result()['validation_1'][config['eval_metric']]))\n",
        "    print(\"model.best_ntree_limit: \", model.best_ntree_limit)\n",
        "    \n",
        "    # print('train_label[:10]: ', train_label[:10])\n",
        "    # print('val_label[:10]: ', val_label[:10])\n",
        "    \n",
        "    train_pred_probs = model.predict_proba(train_data, ntree_limit=model.best_ntree_limit)\n",
        "    val_pred_probs = model.predict_proba(val_data, ntree_limit=model.best_ntree_limit)\n",
        "    # print(\"train_pred_probs.shape: \", train_pred_probs.shape)\n",
        "    # print(\"val_pred_probs.shape: \", val_pred_probs.shape)\n",
        "    # print(\"train_pred_probs[:10]: \", train_pred_probs[:10])\n",
        "    # print(\"val_pred_probs[:10]: \", val_pred_probs[:10])\n",
        "    \n",
        "    train_pred_cls = np.argmax(train_pred_probs, axis=1)\n",
        "    # train_pred_cls = np.copy(train_pred_probs)\n",
        "    # train_pred_cls[train_pred_cls <= 0.5] = 0\n",
        "    # train_pred_cls[train_pred_cls > 0.5] = 1\n",
        "    \n",
        "    val_pred_cls = np.argmax(val_pred_probs, axis=1)\n",
        "    # val_pred_cls = np.copy(val_pred_probs)\n",
        "    # val_pred_cls[val_pred_cls <= 0.5] = 0\n",
        "    # val_pred_cls[val_pred_cls > 0.5] = 1\n",
        "\n",
        "    # print(\"train_pred_cls.shape: \", train_pred_cls.shape)\n",
        "    # print(\"val_pred_cls.shape: \", val_pred_cls.shape)\n",
        "\n",
        "    train_pred_cont = train_pred_probs[:, 1]\n",
        "    val_pred_cont = val_pred_probs[:, 1]\n",
        "    # print(\"train_pred_cont.shape: \", train_pred_cont.shape)\n",
        "    # print(\"val_preds_cont.shape: \", val_preds_cont.shape)\n",
        "\n",
        "    train_auroc = roc_auc_score(train_label.values, train_pred_cont)\n",
        "    val_auroc = roc_auc_score(val_label.values, val_pred_cont)\n",
        "\n",
        "    train_f1 = f1_score(train_label.values, train_pred_cls)\n",
        "    val_f1 = f1_score(val_label.values, val_pred_cls)\n",
        "    \n",
        "    train_logloss = log_loss(train_label.values, train_pred_probs)\n",
        "    val_logloss = log_loss(val_label.values, val_pred_probs)\n",
        "    \n",
        "    per_model_metrics['AUROC']['train'] = train_auroc\n",
        "    per_model_metrics['AUROC']['val'] = val_auroc\n",
        "    per_model_metrics['f1_score']['train'] = train_f1\n",
        "    per_model_metrics['f1_score']['val'] = val_f1\n",
        "    per_model_metrics['logloss']['train'] = train_logloss\n",
        "    per_model_metrics['logloss']['val'] = val_logloss\n",
        "\n",
        "    print(f\"AUROC: Train: {train_auroc} \\t Val: {val_auroc}\")\n",
        "    print(f\"F1 score: Train: {train_f1} \\t Val: {val_f1}\")\n",
        "    print(f\"Log loss: Train: {train_logloss} \\t Val: {val_logloss}\")\n",
        "    \n",
        "    # wandb.log({\n",
        "    #     \"Best Epoch Train AUROC\": train_auroc, \n",
        "    #     \"Best Epoch Val AUROC\": val_auroc,\n",
        "\n",
        "    #     \"Best Epoch Train F1\": train_f1, \n",
        "    #     \"Best Epoch Val F1\": val_f1,\n",
        "        \n",
        "    #     \"Best Epoch Train Log Loss\": train_logloss, \n",
        "    #     \"Best Epoch Val Log Loss\": val_logloss,\n",
        "    #     }\n",
        "    #   )\n",
        "\n",
        "    print('-'*30)\n",
        "  \n",
        "  print(\"Fold average stats.: \")\n",
        "  print(f\"AUROC: Train: {np.mean(per_model_metrics['AUROC']['train'])} \\t Val: {np.mean(per_model_metrics['AUROC']['val'])}\")\n",
        "  print(f\"F1 score: Train: {np.mean(per_model_metrics['f1_score']['train'])} \\t Val: {np.mean(per_model_metrics['f1_score']['val'])}\")\n",
        "  print(f\"Log loss: Train: {np.mean(per_model_metrics['logloss']['train'])} \\t Val: {np.mean(per_model_metrics['logloss']['val'])}\")\n",
        "\n",
        "  wandb.log({\n",
        "        \"Fold average Train AUROC\": np.mean(per_model_metrics['AUROC']['train']), \n",
        "        \"Fold average Val AUROC\": np.mean(per_model_metrics['AUROC']['val']),\n",
        "        }\n",
        "      )\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEEIFAjRzckV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_paths(config):\n",
        "  config['local_model_dir'] = '/content/model_store'\n",
        "  config['drive_model_dir'] = os.path.join(config['drive_project_dir'], 'model_store')\n",
        "  \n",
        "  if not os.path.exists(config['local_model_dir']):\n",
        "    os.mkdir(config['local_model_dir'])\n",
        "  \n",
        "  if not os.path.exists(config['drive_model_dir']):\n",
        "    os.mkdir(config['drive_model_dir'])\n",
        "  \n",
        "  # -------------\n",
        "\n",
        "  config['local_model_dir'] = os.path.join(config['local_model_dir'], config['model_name']) \n",
        "  config['drive_model_dir'] = os.path.join(config['drive_model_dir'], config['model_name']) \n",
        "\n",
        "  if not os.path.exists(config['local_model_dir']): \n",
        "    os.mkdir(config['local_model_dir'])\n",
        "  if not os.path.exists(config['drive_model_dir']): \n",
        "    os.mkdir(config['drive_model_dir'])\n",
        "  \n",
        "  # -------------\n",
        "\n",
        "  config['local_data_dir'] = '/content/data'\n",
        "  config['drive_data_dir'] = os.path.join(config['drive_project_dir'], 'data/comp_data')\n",
        "\n",
        "  config['local_feature_dir'] = '/content/feature_store'\n",
        "  config['drive_feature_dir'] = os.path.join(config['drive_project_dir'], 'feature_store')\n",
        "\n",
        "  if not os.path.exists(config['local_data_dir']):\n",
        "    os.mkdir(config['local_data_dir'])\n",
        "  \n",
        "  if not os.path.exists(config['local_feature_dir']):\n",
        "    os.mkdir(config['local_feature_dir'])\n",
        "\n"
      ],
      "metadata": {
        "id": "cAhSGQuxzchu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_model(config):\n",
        "  create_paths(config)\n",
        "  \n",
        "  \n",
        "  train_df, test_df, sub_df = get_data(config)\n",
        "  print(\"train_df.shape: \", train_df.shape)\n",
        "\n",
        "  test_ids = test_df.id.values\n",
        "  \n",
        "  train_df, test_df = encode_features(config, train_df, test_df)\n",
        "\n",
        "  config = get_feature_cols(config, train_df)\n",
        "\n",
        "  # ------------------\n",
        "  \n",
        "  # float_feature_cols = [col for col in config['feature_cols'] if col not in ['Latitude', 'Longitude', 'MedHouseVal']]\n",
        "\n",
        "  # if config['handle_outliers']:\n",
        "  #   # handle outliers\n",
        "  #   print(\"Before outlier handling: \")\n",
        "  #   print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  #   train_df, test_df = handle_outliers(config, train_df, test_df, cols=float_feature_cols, method='iqr')\n",
        "  #   print(\"After outlier handling: \")\n",
        "  #   print(f\"train_df.shape: {train_df.shape} \\t test_df.shape: {test_df.shape}\")\n",
        "  \n",
        "  # if config['scale_data']:\n",
        "  #   print(\"Scaling data...\")\n",
        "  #   # scale data\n",
        "  #   # float_feature_cols = [col for col in config['feature_cols'] if col not in ['Latitude', 'Longitude', 'MedHouseVal']]\n",
        "  #   train_df, test_df = scale_data(train_df, test_df, cols=float_feature_cols, method='robust')\n",
        "    \n",
        "\n",
        "  # ------------------\n",
        "\n",
        "  fold_idx_dict = generate_fold_idx(config, train_df)\n",
        "  \n",
        "  per_model_metrics = {\n",
        "                        'AUROC': {'train': [], 'val': []},\n",
        "                        'f1_score': {'train': [], 'val': []},\n",
        "                        'logloss': {'train': [], 'val': []},\n",
        "                      }\n",
        "\n",
        "  per_fold_test_pred_probs = None\n",
        "  per_fold_test_pred_cls = None\n",
        "  pred_cnt = 0\n",
        "\n",
        "  for fold_num in range(config['num_folds']):\n",
        "    if fold_num not in config['folds_to_train']:\n",
        "      continue\n",
        "    pred_cnt += 1\n",
        "    print(\"Training fold: \", fold_num)\n",
        "    config['curr_fold'] = fold_num\n",
        "\n",
        "    # -----------\n",
        "    fold_model_name = config['model_name'] + '_fold_' + str(config['curr_fold'])\n",
        "\n",
        "    train_idx = fold_idx_dict[fold_num]['train_idx']\n",
        "    val_idx = fold_idx_dict[fold_num]['val_idx']\n",
        "    print(\"len(train_idx): {} \\t len(val_idx): {}\".format(len(train_idx), len(val_idx)))\n",
        "    \n",
        "    train_data = train_df[config['feature_cols']].iloc[train_idx]\n",
        "    train_label = train_df[config['target_cols']].iloc[train_idx]\n",
        "\n",
        "    val_data = train_df[config['feature_cols']].iloc[val_idx]\n",
        "    val_label = train_df[config['target_cols']].iloc[val_idx]\n",
        "\n",
        "    test_data = test_df[config['feature_cols']]\n",
        "    \n",
        "    # --------\n",
        "\n",
        "    model = load_model(config)\n",
        "\n",
        "    train_pred_probs = model.predict_proba(train_data, ntree_limit=model.best_ntree_limit)\n",
        "    val_pred_probs = model.predict_proba(val_data, ntree_limit=model.best_ntree_limit)\n",
        "    test_pred_probs = model.predict_proba(test_data, ntree_limit=model.best_ntree_limit)\n",
        "\n",
        "    # train_pred_cls = np.copy(train_pred_probs)\n",
        "    # train_pred_cls[train_pred_cls <= 0.5] = 0\n",
        "    # train_pred_cls[train_pred_cls > 0.5] = 1\n",
        "    \n",
        "    # val_pred_cls = np.copy(val_pred_probs)\n",
        "    # val_pred_cls[val_pred_cls <= 0.5] = 0\n",
        "    # val_pred_cls[val_pred_cls > 0.5] = 1\n",
        "\n",
        "    # test_pred_cls = np.copy(test_pred_probs)\n",
        "    # test_pred_cls[test_pred_cls <= 0.5] = 0\n",
        "    # test_pred_cls[test_pred_cls > 0.5] = 1\n",
        "\n",
        "    train_pred_cls = np.argmax(train_pred_probs, axis=1)\n",
        "    val_pred_cls = np.argmax(val_pred_probs, axis=1)\n",
        "    test_pred_cls = np.argmax(test_pred_probs, axis=1)\n",
        "\n",
        "    train_pred_cont = train_pred_probs[:, 1]\n",
        "    val_pred_cont = val_pred_probs[:, 1]\n",
        "    test_pred_cont = test_pred_probs[:, 1]\n",
        "    \n",
        "    # --------\n",
        "\n",
        "    train_auroc = roc_auc_score(train_label.values, train_pred_cont)\n",
        "    val_auroc = roc_auc_score(val_label.values, val_pred_cont)\n",
        "    train_f1 = f1_score(train_label.values, train_pred_cls)\n",
        "    val_f1 = f1_score(val_label.values, val_pred_cls)\n",
        "    train_logloss = log_loss(train_label.values, train_pred_probs)\n",
        "    val_logloss = log_loss(val_label.values, val_pred_probs)\n",
        "    \n",
        "    per_model_metrics['AUROC']['train'] = train_auroc\n",
        "    per_model_metrics['AUROC']['val'] = val_auroc\n",
        "    per_model_metrics['f1_score']['train'] = train_f1\n",
        "    per_model_metrics['f1_score']['val'] = val_f1\n",
        "    per_model_metrics['logloss']['train'] = train_logloss\n",
        "    per_model_metrics['logloss']['val'] = val_logloss\n",
        "\n",
        "    print(f\"AUROC: Train: {train_auroc} \\t Val: {val_auroc}\")\n",
        "    print(f\"F1 score: Train: {train_f1} \\t Val: {val_f1}\")\n",
        "    print(f\"Log loss: Train: {train_logloss} \\t Val: {val_logloss}\")\n",
        "    \n",
        "    \n",
        "    # test_pred_probs = test_pred_probs.flatten()\n",
        "    test_pred_cls = np.reshape(test_pred_cls, newshape=(test_pred_cls.shape[0], 1))\n",
        "    # if per_fold_test_pred_probs is None:\n",
        "    #   per_fold_test_pred_probs = test_pred_probs\n",
        "    #   per_fold_test_pred_cls = test_pred_cls\n",
        "    # else:\n",
        "    #   per_fold_test_pred_probs = np.concatenate((per_fold_test_pred_probs, test_pred_probs), axis=1)\n",
        "    #   per_fold_test_pred_cls = np.concatenate((per_fold_test_pred_cls, test_pred_cls), axis=1)\n",
        "    if per_fold_test_pred_probs is None:\n",
        "      per_fold_test_pred_probs = test_pred_probs\n",
        "      per_fold_test_pred_cls = test_pred_cls\n",
        "    else:\n",
        "      per_fold_test_pred_probs += test_pred_probs\n",
        "      per_fold_test_pred_cls = np.concatenate((per_fold_test_pred_cls, test_pred_cls), axis=1)\n",
        "\n",
        "\n",
        "  print(\"Fold average stats.: \")\n",
        "  print(f\"AUROC: Train: {np.mean(per_model_metrics['AUROC']['train'])} \\t Val: {np.mean(per_model_metrics['AUROC']['val'])}\")\n",
        "  print(f\"F1 score: Train: {np.mean(per_model_metrics['f1_score']['train'])} \\t Val: {np.mean(per_model_metrics['f1_score']['val'])}\")\n",
        "  print(f\"Log loss: Train: {np.mean(per_model_metrics['logloss']['train'])} \\t Val: {np.mean(per_model_metrics['logloss']['val'])}\")\n",
        "\n",
        "  print(\"per_fold_test_pred_probs.shape: \", per_fold_test_pred_probs.shape)\n",
        "  print(\"per_fold_test_pred_cls.shape: \", per_fold_test_pred_cls.shape)\n",
        "\n",
        "  # avg_test_pred_probs = np.mean(per_fold_test_pred_probs, axis=1).flatten()\n",
        "  avg_test_pred_probs = per_fold_test_pred_probs / pred_cnt\n",
        "  mode_test_pred_cls, _ = mode(per_fold_test_pred_cls, axis=1)\n",
        "  mode_test_pred_cls = mode_test_pred_cls.flatten()\n",
        "  print(\"avg_test_pred_probs.shape: \", avg_test_pred_probs.shape)\n",
        "  print(\"mode_test_pred_cls.shape: \", mode_test_pred_cls.shape)\n",
        "  print(\"avg_test_pred_probs[:10]: \", avg_test_pred_probs[:10])\n",
        "  print(\"mode_test_pred_cls[:10]: \", mode_test_pred_cls[:10])\n",
        "  \n",
        "  sub_df = pd.DataFrame([])\n",
        "  sub_df['id'] = test_ids # test_df.id.values\n",
        "  sub_df['Attrition'] = avg_test_pred_probs[:, 1]\n",
        "  sub_df.to_csv(os.path.join(config['local_model_dir'], 'sample_submission.csv'), index=False)\n",
        "  shutil.copy(os.path.join(config['local_model_dir'], 'sample_submission.csv'), \n",
        "              os.path.join(config['drive_model_dir'], 'sample_submission.csv'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p02CccbvRZPI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " SWEEP best config: \n",
        "\n",
        "max_depth': 2, \n",
        "    'max_leaves': 8,  \n",
        "    'learning_rate': 0.1, \n",
        "\n",
        "    'reg_alpha': 1.0, \n",
        "    'reg_lambda': 5.0, \n",
        "'colsample_bytree': 0.8, \n",
        "    'subsample': 0.8, \n",
        "'''"
      ],
      "metadata": {
        "id": "5X4KClnCzce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a21219f-bd38-4fca-e43c-30e3fcaffcf2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n SWEEP best config: \\n\\nmax_depth': 2, \\n    'max_leaves': 8,  \\n    'learning_rate': 0.1, \\n\\n    'reg_alpha': 1.0, \\n    'reg_lambda': 5.0, \\n'colsample_bytree': 0.8, \\n    'subsample': 0.8, \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = {\n",
        "    'choice': 3, \n",
        "    'random_state': 42, \n",
        "    \n",
        "    'model_name': 'xgb_model_sweep_4', \n",
        "    \n",
        "    'feature_version': 'v0',  # v0 (means original data only) / v1 (orig + extracted features on comp data) / v2: (orig + extracted features on comp + orig data)\n",
        "    'include_orig_data': True, \n",
        "    'handle_outliers': False, \n",
        "    'scale_data': False, \n",
        "    \n",
        "    'enable_categorical': False, \n",
        "    # 'num_classes': 2, \n",
        "    'fold_split_type': 'strat_kfold', #'kfold', \n",
        "    'num_folds': 10, \n",
        "    'folds_to_train': [0,1,2,3,4,5,6,7,8,9,10], \n",
        "    \n",
        "    'tree_method': 'hist', \n",
        "    'n_estimators': 9999, \n",
        "    'early_stopping_rounds': 200, \n",
        "\n",
        "    'colsample_bytree': 0.8, \n",
        "    'subsample': 0.6, \n",
        "    \n",
        "    'max_depth': 5, \n",
        "    'max_leaves': 12,  \n",
        "    'learning_rate': 0.03, \n",
        "\n",
        "    'reg_alpha': 0.5, \n",
        "    'reg_lambda': 3.0, \n",
        "\n",
        "    # if 'auto'; will be overridden as sum(negative instances) / sum(positive instances)\n",
        "    # else; provided value will be used.\n",
        "    # with 5 fold strat_kfold; auto pos weight is around 23.1956.\n",
        "    'scale_pos_weight': 'auto', # 'auto',  / 10 / 25 etc.\n",
        "\n",
        "    'verbosity': 1,\n",
        "\n",
        "    'objective': 'binary:logistic', \n",
        "    'eval_metric': 'auc', \n",
        "    \n",
        "    'use_gpu_if_available': True, \n",
        "    'predictor': 'gpu_predictor',\n",
        "    'use_wandb': False, # Defaults to true if choice==3.\n",
        "    'n_jobs': -1, \n",
        "    'data_dir': '/content/data/', \n",
        "    'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E03', \n",
        "    'project_name': 'playground_s03_e03', \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "if config['use_gpu_if_available']:\n",
        "  if torch.cuda.is_available():\n",
        "    config['gpu_id'] = 0\n",
        "    config['tree_method'] = 'gpu_hist'\n",
        "    config['predictor'] = 'gpu_predictor'\n",
        "    print(\"GPU available... XGBoost will use GPU...\")\n",
        "  else:\n",
        "    print(\"NOT USING GPU!!!!!! Parameter 'use_gpu_if_available' is set to TRUE; But NO GPU IS VISIBLE!!!!!\")\n",
        "    if config['tree_method'] == 'gpu_hist': config['tree_method'] = 'hist'\n",
        "    if config['predictor'] == 'gpu_predictor': config['predictor'] = 'cpu_predictor'\n",
        "else:\n",
        "  if config['tree_method'] == 'gpu_hist': config['tree_method'] = 'hist'\n",
        "  if config['predictor'] == 'gpu_predictor': config['predictor'] = 'cpu_predictor' \n",
        "  print(\"NOT USING GPU!!!!!! Parameter 'use_gpu_if_available' is set to False!!!!!!!\")    \n",
        "\n",
        "\n",
        "if config['choice'] == 3: config['use_wandb'] = True\n",
        "\n",
        "if config['use_wandb']:\n",
        "  os.environ['WANDB_MODE'] = 'online'\n",
        "  try: \n",
        "    wandb.login(key='d60ad29783a045de090c17001912975dc8f9f2e2') \n",
        "  except:\n",
        "    wandb.login()\n",
        "# else:\n",
        "# os.environ['WANDB_MODE'] = 'offline'\n",
        "\n",
        "set_seeds(config)\n",
        "\n",
        "if config['choice'] == 1:\n",
        "  train_k_folds()\n",
        "elif config['choice'] == 2:\n",
        "  test_model(config)\n",
        "elif config['choice'] == 3:\n",
        "  sweep_configs = {\n",
        "      \"method\": \"bayes\",\n",
        "      \"metric\": {\n",
        "          # \"name\": \"Best Epoch Val AUROC\",\n",
        "          \"name\": \"Fold average Val AUROC\",\n",
        "          \"goal\": \"maximize\",\n",
        "      },\n",
        "      \"parameters\": {      \n",
        "          \"colsample_bytree\": {\n",
        "              \"values\": [0.6, 0.8, 1.0]\n",
        "          },\n",
        "          \"subsample\": {\n",
        "              \"values\": [0.6, 0.8, 1.0]\n",
        "          },\n",
        "          \"max_depth\": {\n",
        "              \"values\": [2, 4, 6, 8, 12, 24]\n",
        "          },\n",
        "          'max_leaves': {\n",
        "              'values': [2, 4, 8, 16, 32, 64],\n",
        "          },\n",
        "          \"reg_alpha\": {\n",
        "              \"values\": [0, 1.0, 2.0, 5.0]\n",
        "          },\n",
        "          \"reg_lambda\": {\n",
        "              \"values\": [0, 1.0, 2.0, 5.0]\n",
        "          },\n",
        "          \"learning_rate\": {\n",
        "              \"values\": [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "          }\n",
        "          # \"random_state\": {\n",
        "          #     \"values\": [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "          # },\n",
        "        }\n",
        "  }\n",
        "  print(\"Running sweep>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  sweep_id = wandb.sweep(sweep=sweep_configs, project=config['project_name']+'_sweep')\n",
        "  wandb.agent(sweep_id=sweep_id, function=train_k_folds, count=50)\n",
        "else:\n",
        "  raise ValueError(f\"Incorrect value for 'choice'={config['choice']} in config\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MoQ2_0Dbzccr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "164a1685-ccfb-4c7e-9cf9-d133dda26ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT USING GPU!!!!!! Parameter 'use_gpu_if_available' is set to TRUE; But NO GPU IS VISIBLE!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaideepmurkute\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running sweep>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Create sweep with ID: axcnih4g\n",
            "Sweep URL: https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 20iju99j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolsample_bytree: 0.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_leaves: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_lambda: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubsample: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config:  {'choice': 3, 'random_state': 42, 'model_name': 'xgb_model_sweep_4', 'feature_version': 'v0', 'include_orig_data': True, 'handle_outliers': False, 'scale_data': False, 'enable_categorical': False, 'fold_split_type': 'strat_kfold', 'num_folds': 10, 'folds_to_train': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'tree_method': 'hist', 'n_estimators': 9999, 'early_stopping_rounds': 200, 'colsample_bytree': 0.8, 'subsample': 0.6, 'max_depth': 5, 'max_leaves': 12, 'learning_rate': 0.03, 'reg_alpha': 0.5, 'reg_lambda': 3.0, 'scale_pos_weight': 'auto', 'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'use_gpu_if_available': True, 'predictor': 'cpu_predictor', 'use_wandb': True, 'n_jobs': -1, 'data_dir': '/content/data/', 'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E03', 'project_name': 'playground_s03_e03'}\n",
            "train_df.shape:  (3147, 34)\n",
            "missing_cols:  {'Attrition'}\n",
            "config['encoded_nominal_categorical_cols']:  ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes']\n",
            "config['encoded_ordinal_categorical_cols']:  []\n",
            "config['feature_cols']:  ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230118_122713-20iju99j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/20iju99j\" target=\"_blank\">xgb_model_sweep_4</a></strong> to <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/20iju99j\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/20iju99j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating sweep configs...\n",
            "*** Updated sweep config:  {'choice': 3, 'random_state': 42, 'model_name': 'xgb_model_sweep_4', 'feature_version': 'v0', 'include_orig_data': True, 'handle_outliers': False, 'scale_data': False, 'enable_categorical': False, 'fold_split_type': 'strat_kfold', 'num_folds': 10, 'folds_to_train': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'tree_method': 'hist', 'n_estimators': 9999, 'early_stopping_rounds': 200, 'colsample_bytree': 0.8, 'subsample': 0.6, 'max_depth': 2, 'max_leaves': 64, 'learning_rate': 0.5, 'reg_alpha': 1, 'reg_lambda': 0, 'scale_pos_weight': 'auto', 'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'use_gpu_if_available': True, 'predictor': 'cpu_predictor', 'use_wandb': True, 'n_jobs': -1, 'data_dir': '/content/data/', 'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E03', 'project_name': 'playground_s03_e03', 'local_model_dir': '/content/model_store/xgb_model_sweep_4', 'drive_model_dir': '/content/drive/MyDrive/Playground Series/S03_E03/model_store/xgb_model_sweep_4', 'local_data_dir': '/content/data', 'drive_data_dir': '/content/drive/MyDrive/Playground Series/S03_E03/data/comp_data', 'local_feature_dir': '/content/feature_store', 'drive_feature_dir': '/content/drive/MyDrive/Playground Series/S03_E03/feature_store', 'nominal_categorical_cols': ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime'], 'ordinal_categorical_cols': ['BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance'], 'ordinal_continuous_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'encoded_nominal_categorical_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'encoded_ordinal_categorical_cols': [], 'id_cols': ['id'], 'cont_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'cat_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'target_cols': ['Attrition'], 'feature_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes']}\n",
            "Training fold:  0\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.69678\tvalidation_1-auc:0.71532\n",
            "[50]\tvalidation_0-auc:0.89209\tvalidation_1-auc:0.80686\n",
            "[100]\tvalidation_0-auc:0.93024\tvalidation_1-auc:0.77524\n",
            "[150]\tvalidation_0-auc:0.95032\tvalidation_1-auc:0.77340\n",
            "[200]\tvalidation_0-auc:0.96778\tvalidation_1-auc:0.77407\n",
            "[231]\tvalidation_0-auc:0.97394\tvalidation_1-auc:0.78455\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.7153220395840322\n",
            "model.best_ntree_limit:  33\n",
            "AUROC: Train: 0.8723572731910525 \t Val: 0.8227104998322711\n",
            "F1 score: Train: 0.5103906899418121 \t Val: 0.5076923076923077\n",
            "Log loss: Train: 0.4524213522456841 \t Val: 0.46556497759644\n",
            "------------------------------\n",
            "Training fold:  1\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.70012\tvalidation_1-auc:0.69146\n",
            "[50]\tvalidation_0-auc:0.89389\tvalidation_1-auc:0.82766\n",
            "[100]\tvalidation_0-auc:0.92798\tvalidation_1-auc:0.85340\n",
            "[150]\tvalidation_0-auc:0.95018\tvalidation_1-auc:0.85340\n",
            "[200]\tvalidation_0-auc:0.96688\tvalidation_1-auc:0.85844\n",
            "[250]\tvalidation_0-auc:0.97632\tvalidation_1-auc:0.85819\n",
            "[300]\tvalidation_0-auc:0.98503\tvalidation_1-auc:0.83714\n",
            "[350]\tvalidation_0-auc:0.98949\tvalidation_1-auc:0.83655\n",
            "[400]\tvalidation_0-auc:0.99190\tvalidation_1-auc:0.82825\n",
            "[436]\tvalidation_0-auc:0.99367\tvalidation_1-auc:0.81810\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6914625964441462\n",
            "model.best_ntree_limit:  237\n",
            "AUROC: Train: 0.9737962519574306 \t Val: 0.8731969137873197\n",
            "F1 score: Train: 0.7314285714285714 \t Val: 0.5454545454545454\n",
            "Log loss: Train: 0.24856505447779084 \t Val: 0.39769116887983974\n",
            "------------------------------\n",
            "Training fold:  2\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.68049\tvalidation_1-auc:0.62324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.89717\tvalidation_1-auc:0.81097\n",
            "[100]\tvalidation_0-auc:0.93563\tvalidation_1-auc:0.78900\n",
            "[150]\tvalidation_0-auc:0.95607\tvalidation_1-auc:0.78153\n",
            "[200]\tvalidation_0-auc:0.96926\tvalidation_1-auc:0.76786\n",
            "[244]\tvalidation_0-auc:0.97712\tvalidation_1-auc:0.76426\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6232388460248239\n",
            "model.best_ntree_limit:  46\n",
            "AUROC: Train: 0.8914678459761697 \t Val: 0.8219557195571956\n",
            "F1 score: Train: 0.5256087321578506 \t Val: 0.46875000000000006\n",
            "Log loss: Train: 0.41880938718246286 \t Val: 0.45950231753762755\n",
            "------------------------------\n",
            "Training fold:  3\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.70449\tvalidation_1-auc:0.68165\n",
            "[50]\tvalidation_0-auc:0.89055\tvalidation_1-auc:0.80820\n",
            "[100]\tvalidation_0-auc:0.92999\tvalidation_1-auc:0.79688\n",
            "[150]\tvalidation_0-auc:0.95030\tvalidation_1-auc:0.80694\n",
            "[200]\tvalidation_0-auc:0.96653\tvalidation_1-auc:0.79160\n",
            "[214]\tvalidation_0-auc:0.96973\tvalidation_1-auc:0.79680\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6816504528681651\n",
            "model.best_ntree_limit:  16\n",
            "AUROC: Train: 0.8410545555837238 \t Val: 0.8435927541093593\n",
            "F1 score: Train: 0.46493902439024387 \t Val: 0.48611111111111105\n",
            "Log loss: Train: 0.504213881612779 \t Val: 0.49279798107842604\n",
            "------------------------------\n",
            "Training fold:  4\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.70193\tvalidation_1-auc:0.65616\n",
            "[50]\tvalidation_0-auc:0.89244\tvalidation_1-auc:0.74916\n",
            "[100]\tvalidation_0-auc:0.93175\tvalidation_1-auc:0.72769\n",
            "[150]\tvalidation_0-auc:0.95444\tvalidation_1-auc:0.71335\n",
            "[200]\tvalidation_0-auc:0.97011\tvalidation_1-auc:0.70832\n",
            "[250]\tvalidation_0-auc:0.97931\tvalidation_1-auc:0.74187\n",
            "[258]\tvalidation_0-auc:0.97992\tvalidation_1-auc:0.74673\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6561556524656156\n",
            "model.best_ntree_limit:  60\n",
            "AUROC: Train: 0.898987717612545 \t Val: 0.7548641395504865\n",
            "F1 score: Train: 0.5544217687074829 \t Val: 0.40298507462686567\n",
            "Log loss: Train: 0.4060628720487078 \t Val: 0.5150598233351336\n",
            "------------------------------\n",
            "Training fold:  5\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.70635\tvalidation_1-auc:0.73532\n",
            "[50]\tvalidation_0-auc:0.89786\tvalidation_1-auc:0.73490\n",
            "[100]\tvalidation_0-auc:0.93467\tvalidation_1-auc:0.71302\n",
            "[150]\tvalidation_0-auc:0.96026\tvalidation_1-auc:0.71310\n",
            "[200]\tvalidation_0-auc:0.97250\tvalidation_1-auc:0.71503\n",
            "[210]\tvalidation_0-auc:0.97552\tvalidation_1-auc:0.71125\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6880241529688024\n",
            "model.best_ntree_limit:  11\n",
            "AUROC: Train: 0.821376966950331 \t Val: 0.7835877222408588\n",
            "F1 score: Train: 0.44427123928293066 \t Val: 0.4296296296296296\n",
            "Log loss: Train: 0.5282088627342902 \t Val: 0.5161025182713592\n",
            "------------------------------\n",
            "Training fold:  6\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.69396\tvalidation_1-auc:0.70018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.89721\tvalidation_1-auc:0.75243\n",
            "[100]\tvalidation_0-auc:0.93658\tvalidation_1-auc:0.71411\n",
            "[150]\tvalidation_0-auc:0.95609\tvalidation_1-auc:0.69339\n",
            "[200]\tvalidation_0-auc:0.97164\tvalidation_1-auc:0.71117\n",
            "[211]\tvalidation_0-auc:0.97457\tvalidation_1-auc:0.70924\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6895337135189533\n",
            "model.best_ntree_limit:  13\n",
            "AUROC: Train: 0.833717777381336 \t Val: 0.7939869171418987\n",
            "F1 score: Train: 0.461183704842429 \t Val: 0.41958041958041953\n",
            "Log loss: Train: 0.510545631356953 \t Val: 0.5463243047751132\n",
            "------------------------------\n",
            "Training fold:  7\n",
            "len(train_idx): 2833 \t len(val_idx): 314\n",
            "Setting scale_pos_weight to:  6.190355329949239\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.70518\tvalidation_1-auc:0.65623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.90561\tvalidation_1-auc:0.68446\n",
            "[100]\tvalidation_0-auc:0.93399\tvalidation_1-auc:0.67476\n",
            "[150]\tvalidation_0-auc:0.95504\tvalidation_1-auc:0.68858\n",
            "[200]\tvalidation_0-auc:0.96898\tvalidation_1-auc:0.68849\n",
            "[250]\tvalidation_0-auc:0.97834\tvalidation_1-auc:0.67184\n",
            "[300]\tvalidation_0-auc:0.98422\tvalidation_1-auc:0.65974\n",
            "[350]\tvalidation_0-auc:0.99017\tvalidation_1-auc:0.65048\n",
            "[364]\tvalidation_0-auc:0.99120\tvalidation_1-auc:0.64387\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6429245687805716\n",
            "model.best_ntree_limit:  166\n",
            "AUROC: Train: 0.9607644807412542 \t Val: 0.7053977516519352\n",
            "F1 score: Train: 0.6916588566073103 \t Val: 0.3770491803278689\n",
            "Log loss: Train: 0.2776650026826599 \t Val: 0.5967726128175947\n",
            "------------------------------\n",
            "Training fold:  8\n",
            "len(train_idx): 2833 \t len(val_idx): 314\n",
            "Setting scale_pos_weight to:  6.190355329949239\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.70192\tvalidation_1-auc:0.70411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.90311\tvalidation_1-auc:0.75200\n",
            "[100]\tvalidation_0-auc:0.93335\tvalidation_1-auc:0.75680\n",
            "[150]\tvalidation_0-auc:0.95571\tvalidation_1-auc:0.72985\n",
            "[200]\tvalidation_0-auc:0.96718\tvalidation_1-auc:0.72994\n",
            "[206]\tvalidation_0-auc:0.96907\tvalidation_1-auc:0.72548\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.7041105294773878\n",
            "model.best_ntree_limit:  7\n",
            "AUROC: Train: 0.8050206771103245 \t Val: 0.782673989530593\n",
            "F1 score: Train: 0.4411326378539494 \t Val: 0.40243902439024387\n",
            "Log loss: Train: 0.5464793506784886 \t Val: 0.5857526584273312\n",
            "------------------------------\n",
            "Training fold:  9\n",
            "len(train_idx): 2833 \t len(val_idx): 314\n",
            "Setting scale_pos_weight to:  6.190355329949239\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.70170\tvalidation_1-auc:0.71437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.90250\tvalidation_1-auc:0.75732\n",
            "[100]\tvalidation_0-auc:0.94119\tvalidation_1-auc:0.76186\n",
            "[150]\tvalidation_0-auc:0.95940\tvalidation_1-auc:0.75122\n",
            "[200]\tvalidation_0-auc:0.97195\tvalidation_1-auc:0.73363\n",
            "[250]\tvalidation_0-auc:0.98093\tvalidation_1-auc:0.76375\n",
            "[277]\tvalidation_0-auc:0.98433\tvalidation_1-auc:0.75388\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.7143653994679482\n",
            "model.best_ntree_limit:  78\n",
            "AUROC: Train: 0.92721386604729 \t Val: 0.7755084527589462\n",
            "F1 score: Train: 0.5965811965811966 \t Val: 0.4159999999999999\n",
            "Log loss: Train: 0.3683417754415925 \t Val: 0.498010467734391\n",
            "------------------------------\n",
            "Fold average stats.: \n",
            "AUROC: Train: 0.92721386604729 \t Val: 0.7755084527589462\n",
            "F1 score: Train: 0.5965811965811966 \t Val: 0.4159999999999999\n",
            "Log loss: Train: 0.3683417754415925 \t Val: 0.498010467734391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Fold average Train AUROC</td><td></td></tr><tr><td>Fold average Val AUROC</td><td></td></tr><tr><td>Per Epoch Train AUC </td><td></td></tr><tr><td>Per Epoch Val AUC</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Fold average Train AUROC</td><td>0.92721</td></tr><tr><td>Fold average Val AUROC</td><td>0.77551</td></tr><tr><td>Per Epoch Train AUC </td><td>0.98433</td></tr><tr><td>Per Epoch Val AUC</td><td>0.75388</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">xgb_model_sweep_4</strong> at: <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/20iju99j\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/20iju99j</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230118_122713-20iju99j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mak1c4wv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolsample_bytree: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_leaves: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_alpha: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_lambda: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubsample: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config:  {'choice': 3, 'random_state': 42, 'model_name': 'xgb_model_sweep_4', 'feature_version': 'v0', 'include_orig_data': True, 'handle_outliers': False, 'scale_data': False, 'enable_categorical': False, 'fold_split_type': 'strat_kfold', 'num_folds': 10, 'folds_to_train': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'tree_method': 'hist', 'n_estimators': 9999, 'early_stopping_rounds': 200, 'colsample_bytree': 0.8, 'subsample': 0.6, 'max_depth': 2, 'max_leaves': 64, 'learning_rate': 0.5, 'reg_alpha': 1, 'reg_lambda': 0, 'scale_pos_weight': 'auto', 'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'use_gpu_if_available': True, 'predictor': 'cpu_predictor', 'use_wandb': True, 'n_jobs': -1, 'data_dir': '/content/data/', 'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E03', 'project_name': 'playground_s03_e03', 'local_model_dir': '/content/model_store/xgb_model_sweep_4', 'drive_model_dir': '/content/drive/MyDrive/Playground Series/S03_E03/model_store/xgb_model_sweep_4', 'local_data_dir': '/content/data', 'drive_data_dir': '/content/drive/MyDrive/Playground Series/S03_E03/data/comp_data', 'local_feature_dir': '/content/feature_store', 'drive_feature_dir': '/content/drive/MyDrive/Playground Series/S03_E03/feature_store', 'nominal_categorical_cols': ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime'], 'ordinal_categorical_cols': ['BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance'], 'ordinal_continuous_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'encoded_nominal_categorical_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'encoded_ordinal_categorical_cols': [], 'id_cols': ['id'], 'cont_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'cat_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'target_cols': ['Attrition'], 'feature_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'curr_fold': 9}\n",
            "train_df.shape:  (3147, 34)\n",
            "missing_cols:  {'Attrition'}\n",
            "config['encoded_nominal_categorical_cols']:  ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes']\n",
            "config['encoded_ordinal_categorical_cols']:  []\n",
            "config['feature_cols']:  ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230118_122814-mak1c4wv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/mak1c4wv\" target=\"_blank\">xgb_model_sweep_4</a></strong> to <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/sweeps/axcnih4g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/mak1c4wv\" target=\"_blank\">https://wandb.ai/jaideepmurkute/playground_s03_e03_sweep/runs/mak1c4wv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating sweep configs...\n",
            "*** Updated sweep config:  {'choice': 3, 'random_state': 42, 'model_name': 'xgb_model_sweep_4', 'feature_version': 'v0', 'include_orig_data': True, 'handle_outliers': False, 'scale_data': False, 'enable_categorical': False, 'fold_split_type': 'strat_kfold', 'num_folds': 10, 'folds_to_train': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'tree_method': 'hist', 'n_estimators': 9999, 'early_stopping_rounds': 200, 'colsample_bytree': 1, 'subsample': 0.6, 'max_depth': 4, 'max_leaves': 2, 'learning_rate': 0.05, 'reg_alpha': 2, 'reg_lambda': 0, 'scale_pos_weight': 'auto', 'verbosity': 1, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'use_gpu_if_available': True, 'predictor': 'cpu_predictor', 'use_wandb': True, 'n_jobs': -1, 'data_dir': '/content/data/', 'drive_project_dir': '/content/drive/MyDrive/Playground Series/S03_E03', 'project_name': 'playground_s03_e03', 'local_model_dir': '/content/model_store/xgb_model_sweep_4', 'drive_model_dir': '/content/drive/MyDrive/Playground Series/S03_E03/model_store/xgb_model_sweep_4', 'local_data_dir': '/content/data', 'drive_data_dir': '/content/drive/MyDrive/Playground Series/S03_E03/data/comp_data', 'local_feature_dir': '/content/feature_store', 'drive_feature_dir': '/content/drive/MyDrive/Playground Series/S03_E03/feature_store', 'nominal_categorical_cols': ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'OverTime'], 'ordinal_categorical_cols': ['BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance'], 'ordinal_continuous_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'encoded_nominal_categorical_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'encoded_ordinal_categorical_cols': [], 'id_cols': ['id'], 'cont_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'], 'cat_cols': ['Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'target_cols': ['Attrition'], 'feature_cols': ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Department_Human Resources', 'Department_Research & Development', 'Department_Sales', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'Over18_Y', 'OverTime_No', 'OverTime_Yes'], 'curr_fold': 9}\n",
            "Training fold:  0\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.60242\tvalidation_1-auc:0.65318\n",
            "[50]\tvalidation_0-auc:0.78508\tvalidation_1-auc:0.78007\n",
            "[100]\tvalidation_0-auc:0.80296\tvalidation_1-auc:0.79415\n",
            "[150]\tvalidation_0-auc:0.81373\tvalidation_1-auc:0.79780\n",
            "[200]\tvalidation_0-auc:0.82369\tvalidation_1-auc:0.80200\n",
            "[250]\tvalidation_0-auc:0.82983\tvalidation_1-auc:0.80552\n",
            "[300]\tvalidation_0-auc:0.83433\tvalidation_1-auc:0.80644\n",
            "[350]\tvalidation_0-auc:0.83888\tvalidation_1-auc:0.80636\n",
            "[400]\tvalidation_0-auc:0.84170\tvalidation_1-auc:0.80476\n",
            "[450]\tvalidation_0-auc:0.84500\tvalidation_1-auc:0.80686\n",
            "[500]\tvalidation_0-auc:0.84794\tvalidation_1-auc:0.80611\n",
            "[550]\tvalidation_0-auc:0.85014\tvalidation_1-auc:0.80787\n",
            "[600]\tvalidation_0-auc:0.85221\tvalidation_1-auc:0.80820\n",
            "[650]\tvalidation_0-auc:0.85406\tvalidation_1-auc:0.80820\n",
            "[700]\tvalidation_0-auc:0.85618\tvalidation_1-auc:0.81290\n",
            "[750]\tvalidation_0-auc:0.85862\tvalidation_1-auc:0.81206\n",
            "[800]\tvalidation_0-auc:0.85996\tvalidation_1-auc:0.81240\n",
            "[850]\tvalidation_0-auc:0.86120\tvalidation_1-auc:0.81416\n",
            "[900]\tvalidation_0-auc:0.86295\tvalidation_1-auc:0.81458\n",
            "[950]\tvalidation_0-auc:0.86433\tvalidation_1-auc:0.81466\n",
            "[1000]\tvalidation_0-auc:0.86509\tvalidation_1-auc:0.81474\n",
            "[1050]\tvalidation_0-auc:0.86629\tvalidation_1-auc:0.81474\n",
            "[1100]\tvalidation_0-auc:0.86723\tvalidation_1-auc:0.81466\n",
            "[1150]\tvalidation_0-auc:0.86883\tvalidation_1-auc:0.81449\n",
            "[1200]\tvalidation_0-auc:0.86969\tvalidation_1-auc:0.81273\n",
            "[1218]\tvalidation_0-auc:0.87026\tvalidation_1-auc:0.81206\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6531784636028178\n",
            "model.best_ntree_limit:  1020\n",
            "AUROC: Train: 0.8657054000565451 \t Val: 0.8165883931566588\n",
            "F1 score: Train: 0.49638554216867475 \t Val: 0.4888888888888888\n",
            "Log loss: Train: 0.46859929728598626 \t Val: 0.482809529509691\n",
            "------------------------------\n",
            "Training fold:  1\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.62396\tvalidation_1-auc:0.61074\n",
            "[50]\tvalidation_0-auc:0.78332\tvalidation_1-auc:0.78715\n",
            "[100]\tvalidation_0-auc:0.79889\tvalidation_1-auc:0.80766\n",
            "[150]\tvalidation_0-auc:0.81030\tvalidation_1-auc:0.82481\n",
            "[200]\tvalidation_0-auc:0.81974\tvalidation_1-auc:0.83672\n",
            "[250]\tvalidation_0-auc:0.82521\tvalidation_1-auc:0.84602\n",
            "[300]\tvalidation_0-auc:0.83063\tvalidation_1-auc:0.84913\n",
            "[350]\tvalidation_0-auc:0.83395\tvalidation_1-auc:0.84829\n",
            "[400]\tvalidation_0-auc:0.83736\tvalidation_1-auc:0.85106\n",
            "[450]\tvalidation_0-auc:0.84015\tvalidation_1-auc:0.85282\n",
            "[500]\tvalidation_0-auc:0.84362\tvalidation_1-auc:0.85525\n",
            "[550]\tvalidation_0-auc:0.84533\tvalidation_1-auc:0.85659\n",
            "[600]\tvalidation_0-auc:0.84730\tvalidation_1-auc:0.86104\n",
            "[650]\tvalidation_0-auc:0.84961\tvalidation_1-auc:0.85886\n",
            "[700]\tvalidation_0-auc:0.85135\tvalidation_1-auc:0.86271\n",
            "[750]\tvalidation_0-auc:0.85336\tvalidation_1-auc:0.86431\n",
            "[800]\tvalidation_0-auc:0.85509\tvalidation_1-auc:0.86288\n",
            "[850]\tvalidation_0-auc:0.85673\tvalidation_1-auc:0.86020\n",
            "[900]\tvalidation_0-auc:0.85813\tvalidation_1-auc:0.86389\n",
            "[950]\tvalidation_0-auc:0.85958\tvalidation_1-auc:0.86263\n",
            "[964]\tvalidation_0-auc:0.85999\tvalidation_1-auc:0.86364\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6107430392485743\n",
            "model.best_ntree_limit:  766\n",
            "AUROC: Train: 0.8540140236007957 \t Val: 0.8648943307614895\n",
            "F1 score: Train: 0.4939073923639318 \t Val: 0.49635036496350377\n",
            "Log loss: Train: 0.47605692935380084 \t Val: 0.4675638562570962\n",
            "------------------------------\n",
            "Training fold:  2\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.62232\tvalidation_1-auc:0.62550\n",
            "[50]\tvalidation_0-auc:0.78635\tvalidation_1-auc:0.78392\n",
            "[100]\tvalidation_0-auc:0.80195\tvalidation_1-auc:0.78690\n",
            "[150]\tvalidation_0-auc:0.81203\tvalidation_1-auc:0.80242\n",
            "[200]\tvalidation_0-auc:0.81956\tvalidation_1-auc:0.80938\n",
            "[250]\tvalidation_0-auc:0.82594\tvalidation_1-auc:0.81659\n",
            "[300]\tvalidation_0-auc:0.83116\tvalidation_1-auc:0.82187\n",
            "[350]\tvalidation_0-auc:0.83518\tvalidation_1-auc:0.82665\n",
            "[400]\tvalidation_0-auc:0.83868\tvalidation_1-auc:0.82799\n",
            "[450]\tvalidation_0-auc:0.84209\tvalidation_1-auc:0.83126\n",
            "[500]\tvalidation_0-auc:0.84381\tvalidation_1-auc:0.83370\n",
            "[550]\tvalidation_0-auc:0.84675\tvalidation_1-auc:0.83462\n",
            "[600]\tvalidation_0-auc:0.84961\tvalidation_1-auc:0.83563\n",
            "[650]\tvalidation_0-auc:0.85121\tvalidation_1-auc:0.83571\n",
            "[700]\tvalidation_0-auc:0.85303\tvalidation_1-auc:0.83596\n",
            "[750]\tvalidation_0-auc:0.85481\tvalidation_1-auc:0.83747\n",
            "[800]\tvalidation_0-auc:0.85652\tvalidation_1-auc:0.83764\n",
            "[850]\tvalidation_0-auc:0.85828\tvalidation_1-auc:0.83923\n",
            "[900]\tvalidation_0-auc:0.85945\tvalidation_1-auc:0.83688\n",
            "[931]\tvalidation_0-auc:0.86048\tvalidation_1-auc:0.83688\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6248742032874874\n",
            "model.best_ntree_limit:  732\n",
            "AUROC: Train: 0.8540886172220501 \t Val: 0.8399027172089903\n",
            "F1 score: Train: 0.48421052631578954 \t Val: 0.48226950354609927\n",
            "Log loss: Train: 0.4746582660255796 \t Val: 0.48423638232052324\n",
            "------------------------------\n",
            "Training fold:  3\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.63760\tvalidation_1-auc:0.59200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-auc:0.78129\tvalidation_1-auc:0.78606\n",
            "[100]\tvalidation_0-auc:0.79431\tvalidation_1-auc:0.80573\n",
            "[150]\tvalidation_0-auc:0.80907\tvalidation_1-auc:0.81944\n",
            "[200]\tvalidation_0-auc:0.81990\tvalidation_1-auc:0.82976\n",
            "[250]\tvalidation_0-auc:0.82590\tvalidation_1-auc:0.83680\n",
            "[300]\tvalidation_0-auc:0.83083\tvalidation_1-auc:0.83890\n",
            "[350]\tvalidation_0-auc:0.83590\tvalidation_1-auc:0.84569\n",
            "[400]\tvalidation_0-auc:0.83923\tvalidation_1-auc:0.84971\n",
            "[450]\tvalidation_0-auc:0.84216\tvalidation_1-auc:0.85089\n",
            "[500]\tvalidation_0-auc:0.84499\tvalidation_1-auc:0.85072\n",
            "[550]\tvalidation_0-auc:0.84709\tvalidation_1-auc:0.85466\n",
            "[600]\tvalidation_0-auc:0.84917\tvalidation_1-auc:0.85307\n",
            "[650]\tvalidation_0-auc:0.85108\tvalidation_1-auc:0.85382\n",
            "[700]\tvalidation_0-auc:0.85356\tvalidation_1-auc:0.85206\n",
            "[750]\tvalidation_0-auc:0.85512\tvalidation_1-auc:0.85257\n",
            "[774]\tvalidation_0-auc:0.85498\tvalidation_1-auc:0.85055\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.5919993290841999\n",
            "model.best_ntree_limit:  576\n",
            "AUROC: Train: 0.8483506463563364 \t Val: 0.856340154310634\n",
            "F1 score: Train: 0.48566878980891726 \t Val: 0.4999999999999999\n",
            "Log loss: Train: 0.48888921157962056 \t Val: 0.47032310740342215\n",
            "------------------------------\n",
            "Training fold:  4\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.62449\tvalidation_1-auc:0.60617\n",
            "[50]\tvalidation_0-auc:0.78785\tvalidation_1-auc:0.74337\n",
            "[100]\tvalidation_0-auc:0.80448\tvalidation_1-auc:0.75805\n",
            "[150]\tvalidation_0-auc:0.81787\tvalidation_1-auc:0.76275\n",
            "[200]\tvalidation_0-auc:0.82789\tvalidation_1-auc:0.76811\n",
            "[250]\tvalidation_0-auc:0.83411\tvalidation_1-auc:0.77650\n",
            "[300]\tvalidation_0-auc:0.83822\tvalidation_1-auc:0.77952\n",
            "[350]\tvalidation_0-auc:0.84247\tvalidation_1-auc:0.78346\n",
            "[400]\tvalidation_0-auc:0.84475\tvalidation_1-auc:0.78371\n",
            "[450]\tvalidation_0-auc:0.84829\tvalidation_1-auc:0.79067\n",
            "[500]\tvalidation_0-auc:0.85096\tvalidation_1-auc:0.79135\n",
            "[550]\tvalidation_0-auc:0.85365\tvalidation_1-auc:0.79386\n",
            "[600]\tvalidation_0-auc:0.85479\tvalidation_1-auc:0.79520\n",
            "[650]\tvalidation_0-auc:0.85679\tvalidation_1-auc:0.79545\n",
            "[700]\tvalidation_0-auc:0.85855\tvalidation_1-auc:0.79604\n",
            "[750]\tvalidation_0-auc:0.86075\tvalidation_1-auc:0.79587\n",
            "[800]\tvalidation_0-auc:0.86216\tvalidation_1-auc:0.79545\n",
            "[850]\tvalidation_0-auc:0.86312\tvalidation_1-auc:0.79336\n",
            "[900]\tvalidation_0-auc:0.86475\tvalidation_1-auc:0.79369\n",
            "[950]\tvalidation_0-auc:0.86601\tvalidation_1-auc:0.79856\n",
            "[985]\tvalidation_0-auc:0.86641\tvalidation_1-auc:0.79420\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6061724253606172\n",
            "model.best_ntree_limit:  787\n",
            "AUROC: Train: 0.8617503732289231 \t Val: 0.7988929889298892\n",
            "F1 score: Train: 0.49556093623890235 \t Val: 0.4603174603174603\n",
            "Log loss: Train: 0.46886727315903837 \t Val: 0.5010955383232425\n",
            "------------------------------\n",
            "Training fold:  5\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n",
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-auc:0.63448\tvalidation_1-auc:0.64085\n",
            "[50]\tvalidation_0-auc:0.78690\tvalidation_1-auc:0.77763\n",
            "[100]\tvalidation_0-auc:0.80272\tvalidation_1-auc:0.78807\n",
            "[150]\tvalidation_0-auc:0.81399\tvalidation_1-auc:0.79789\n",
            "[200]\tvalidation_0-auc:0.82495\tvalidation_1-auc:0.79906\n",
            "[250]\tvalidation_0-auc:0.83010\tvalidation_1-auc:0.80023\n",
            "[300]\tvalidation_0-auc:0.83556\tvalidation_1-auc:0.79755\n",
            "[350]\tvalidation_0-auc:0.83942\tvalidation_1-auc:0.79604\n",
            "[385]\tvalidation_0-auc:0.84225\tvalidation_1-auc:0.79504\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.640850385776585\n",
            "model.best_ntree_limit:  187\n",
            "AUROC: Train: 0.8216774279702086 \t Val: 0.8012411942301241\n",
            "F1 score: Train: 0.4624090541632983 \t Val: 0.4511278195488721\n",
            "Log loss: Train: 0.5256886749191599 \t Val: 0.5133797838574364\n",
            "------------------------------\n",
            "Training fold:  6\n",
            "len(train_idx): 2832 \t len(val_idx): 315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting scale_pos_weight to:  6.206106870229007\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.62710\tvalidation_1-auc:0.57103\n",
            "[50]\tvalidation_0-auc:0.78715\tvalidation_1-auc:0.75189\n",
            "[100]\tvalidation_0-auc:0.80135\tvalidation_1-auc:0.76551\n",
            "[150]\tvalidation_0-auc:0.81474\tvalidation_1-auc:0.77344\n",
            "[200]\tvalidation_0-auc:0.82454\tvalidation_1-auc:0.78933\n",
            "[250]\tvalidation_0-auc:0.83111\tvalidation_1-auc:0.78824\n",
            "[300]\tvalidation_0-auc:0.83806\tvalidation_1-auc:0.79185\n",
            "[350]\tvalidation_0-auc:0.84088\tvalidation_1-auc:0.79109\n",
            "[400]\tvalidation_0-auc:0.84435\tvalidation_1-auc:0.79319\n",
            "[450]\tvalidation_0-auc:0.84689\tvalidation_1-auc:0.79571\n",
            "[500]\tvalidation_0-auc:0.84967\tvalidation_1-auc:0.79621\n",
            "[550]\tvalidation_0-auc:0.85231\tvalidation_1-auc:0.79260\n",
            "[600]\tvalidation_0-auc:0.85525\tvalidation_1-auc:0.79084\n",
            "[647]\tvalidation_0-auc:0.85669\tvalidation_1-auc:0.79176\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.5710332103321033\n",
            "model.best_ntree_limit:  448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC: Train: 0.8468858988844341 \t Val: 0.7976350218047635\n",
            "F1 score: Train: 0.4803212851405622 \t Val: 0.4525547445255475\n",
            "Log loss: Train: 0.4915535829449599 \t Val: 0.5326403594324514\n",
            "------------------------------\n",
            "Training fold:  7\n",
            "len(train_idx): 2833 \t len(val_idx): 314\n",
            "Setting scale_pos_weight to:  6.190355329949239\n",
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.63631\tvalidation_1-auc:0.62426\n",
            "[50]\tvalidation_0-auc:0.78493\tvalidation_1-auc:0.75088\n",
            "[100]\tvalidation_0-auc:0.80976\tvalidation_1-auc:0.74273\n",
            "[150]\tvalidation_0-auc:0.82056\tvalidation_1-auc:0.73612\n",
            "[200]\tvalidation_0-auc:0.83001\tvalidation_1-auc:0.73003\n",
            "[247]\tvalidation_0-auc:0.83740\tvalidation_1-auc:0.72488\n",
            "Saving Model...\n",
            "Saving model...\n",
            "Copying model to drive...\n",
            "Best val epoch: min validation_1 auc:  0.6242598472496352\n",
            "model.best_ntree_limit:  48\n",
            "AUROC: Train: 0.7842478297879424 \t Val: 0.7527675276752768\n",
            "F1 score: Train: 0.42668735453840184 \t Val: 0.34285714285714286\n",
            "Log loss: Train: 0.5931712861011443 \t Val: 0.597249832407684\n",
            "------------------------------\n",
            "Training fold:  8\n",
            "len(train_idx): 2833 \t len(val_idx): 314\n",
            "Setting scale_pos_weight to:  6.190355329949239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xgboost/core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model ...\n",
            "[0]\tvalidation_0-auc:0.63905\tvalidation_1-auc:0.59916\n",
            "[50]\tvalidation_0-auc:0.77944\tvalidation_1-auc:0.77547\n",
            "[100]\tvalidation_0-auc:0.80063\tvalidation_1-auc:0.78538\n",
            "[150]\tvalidation_0-auc:0.81484\tvalidation_1-auc:0.79173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise"
      ],
      "metadata": {
        "id": "fs8nMyzizcUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYyxNZX7zcQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_data_df = pd.read_csv('/content/data/orig_data.csv')"
      ],
      "metadata": {
        "id": "A27itG2nzcLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_data_df.Attrition.values == 'Yes'"
      ],
      "metadata": {
        "id": "4x0SQTVyzcH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flCOWKEBzcCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZSOzMNEzb-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UuEelh0azb5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBNavoAMzb0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0icBsbt3zbvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}